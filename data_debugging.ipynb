{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Analysis\n",
    "- What does the dataset look like?\n",
    "- What inconsistencies, anomalies, or patterns do you observe?\n",
    "\n",
    "Check readme.md first!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get to the data.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import json_repair as jr\n",
    "\n",
    "directory = Path('mixed')\n",
    "\n",
    "data = pd.DataFrame\n",
    "\n",
    "\n",
    "# count how many files, make sure count of json matches too. \n",
    "count_total_files = len([f for f in directory.iterdir() if f.is_file()])\n",
    "count_json_files = len(list(directory.glob(\"*.json\")))\n",
    "\n",
    "\n",
    "# a counter for how many files fail to load into the dataframe incase we need to debug.\n",
    "count_correctly_loaded = 0\n",
    "count_cannot_load = 0\n",
    "\n",
    "\n",
    "# basic data structures for the interim import\n",
    "all_data = []\n",
    "failed_files = [] # list: [filepath, python error]\n",
    "\n",
    "\n",
    "# to check if all of the files contain the same number of fields (at root level, ignore nested cols)\n",
    "file_shape = {}\n",
    "\n",
    "\n",
    "def put_to_df(row, all):\n",
    "    flat_data = pd.json_normalize(row, sep='_').to_dict('records')\n",
    "    all.extend(flat_data)\n",
    "    file_shape[str(len(*flat_data))] = file_shape.get(str(len(*flat_data)), 0) + 1\n",
    "    return 1 # simply did not fail, so add 1 to a counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for json_file in directory.glob('*.json'):\n",
    "    with open(json_file) as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            count_correctly_loaded += put_to_df(data, all_data)\n",
    "\n",
    "        # handle non JSON like files into an error list, to process a 2nd time a different way\n",
    "        except json.JSONDecodeError as jde:\n",
    "            failed_files.append([json_file, jde])\n",
    "            continue\n",
    "\n",
    "\n",
    "for f in failed_files: \n",
    "    with open(f[0]) as j:\n",
    "        content = j.read().strip()\n",
    "        p = jr.repair_json(content) # use the json_repair module to auto fix the json file\n",
    "        d = json.loads(p)\n",
    "        count_correctly_loaded += put_to_df(d, all_data)\n",
    "        # we can write the corrected json out to file too, if we wanted to fix source\n",
    "\n",
    "\n",
    "# effectively continue debugging potential file errors until an acceptable amount of data is inside the dataframe. display for a human below.\n",
    "\n",
    "print(f'There are {count_total_files}, of which {count_json_files} are json. {count_correctly_loaded} ingested to dataframe succesfully, {count_cannot_load} could not load.')\n",
    "print(f'The column shape of the json files: {file_shape}')\n",
    "\n",
    "\n",
    "# convert all the json dict objects to a pandas dataframe for easier processing.\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "print(f'Double check DF length: {len(df)}')\n",
    "\n",
    "# This type of error handling can be managed into a task failure notifcation in airflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proof that a failed json file (error file user_7100 below) ingested into the data objects correctly.\n",
    "for x in all_data:\n",
    "    if x['user_id'] == 'd088da63-558d-4008-b378-fde896abafd8':\n",
    "        print(x)\n",
    "\n",
    "df.query('user_id==\"d088da63-558d-4008-b378-fde896abafd8\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the dataset look like?\n",
    "- We have reasonably well structured data, although many fields have poor health that will have to be handled later.\n",
    "- We have 6 consistent fields, in user_id, name, email, instagram_handle, tiktok_handle.\n",
    "  - Later in dbt, Lets check for rows with nonsense data, e.g. no PII, no useful keys, and should be purged. \n",
    "  - We have a nested data structure in advocacy_programs, lets unpack at least that into the main dataframe...\n",
    "- Normalise to per platform for easier mapping and nicer structure?\n",
    "  - At this point, migrate it out of pandas.\n",
    "- ### Explore duplicated user ID's - get most recent?\n",
    "  - Implement dbt snapshot sorted by the date stamp?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets unpack advocacy program into the parent.\n",
    "unpacked = pd.json_normalize(df['advocacy_programs'].explode('advocacy_programs'))\n",
    "newdf = pd.concat([df, unpacked], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tasks = pd.json_normalize(newdf['tasks_completed'].explode('tasks_completed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a docker db for uploading to : postgres?\n",
    "- unpack tasks completed into a seperate table with userid\n",
    "- clean the data up\n",
    "- implement the dbt process - connect to postgres\n",
    "- sort the dockerisation process out for easy reproduction\n",
    "- If it was production:\n",
    "  - Airflow task\n",
    "    - Read the files\n",
    "    - tidy them\n",
    "    - upload to postgres\n",
    "    - run any dbt packages\n",
    "    - create a simple export file from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data somewhere so we can use it in dbt-core: \n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# connect and use the local postgres instance we have running in docker.\n",
    "engine = create_engine('postgresql://postgres:pass@127.0.0.1:5432/duel')\n",
    "\n",
    "# this is safe to do in this instance, because advocacy_programs always has only 1 item in it.\n",
    "newdf[['user_id', 'name', 'email', 'instagram_handle', 'tiktok_handle', 'joined_at']].astype(str).to_sql('source', engine, if_exists='replace', index=True, method='multi', schema='raw')\n",
    "newdf[['program_id', 'brand', 'total_sales_attributed']].astype(str).to_sql('source__advocacy_programs', engine, if_exists='replace', index=True, method='multi', schema='raw')\n",
    "tasks.astype(str).to_sql('source__tasks_completed', engine, if_exists='replace', index=True, method='multi', schema='raw')\n",
    "\n",
    "# df.astype(str).to_sql('source', engine, if_exists='replace', index=True, method='multi', schema='raw')\n",
    "\n",
    "newdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider this stage done. data has been ingested and viewed at a basic level. \n",
    "\n",
    "The data has been expanded first in python for faster and easier json error handling and unpacking. If these json lists were of length > 1, we could handle that by duplicating the index id and having a one to many relationship. However, all lists are length 1 in this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
