{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Analysis\n",
    "- What does the dataset look like?\n",
    "- What inconsistencies, anomalies, or patterns do you observe?\n",
    "\n",
    "1) Please note Python 3.12.2 was used \n",
    "2) to run this notebook in a .venv please pip install from the requirements.txt\n",
    "3) Open the source data file local to this directory: tar xvf data.tar.gz will do what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10000, of which 10000 are json. 10000 ingested to dataframe succesfully, 0 could not load.\n",
      "The column shape of the json files: {'7': 10000}\n",
      "Double check DF length: 10000\n"
     ]
    }
   ],
   "source": [
    "# lets get to the data.\n",
    "# Terminal > tar xvf data.tar.gz\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import json_repair as jr\n",
    "\n",
    "directory = Path('mixed')\n",
    "\n",
    "data = pd.DataFrame\n",
    "\n",
    "\n",
    "# count how many files, make sure count of json matches too. \n",
    "count_total_files = len([f for f in directory.iterdir() if f.is_file()])\n",
    "count_json_files = len(list(directory.glob(\"*.json\")))\n",
    "\n",
    "\n",
    "# a counter for how many files fail to load into the dataframe incase we need to debug.\n",
    "count_correctly_loaded = 0\n",
    "count_cannot_load = 0\n",
    "\n",
    "\n",
    "# basic data structures for the interim import\n",
    "all_data = []\n",
    "failed_files = [] # list: [filepath, python error]\n",
    "\n",
    "\n",
    "# to check if all of the files contain the same number of fields (at root level, ignore nested cols)\n",
    "file_shape = {}\n",
    "\n",
    "\n",
    "def put_to_df(row, all):\n",
    "    flat_data = pd.json_normalize(row, sep='_').to_dict('records')\n",
    "    all.extend(flat_data)\n",
    "    file_shape[str(len(*flat_data))] = file_shape.get(str(len(*flat_data)), 0) + 1\n",
    "    return 1 # simply did not fail, so add 1 to a counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for json_file in directory.glob('*.json'):\n",
    "    with open(json_file) as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            count_correctly_loaded += put_to_df(data, all_data)\n",
    "\n",
    "        # handle non JSON like files into an error list, to process a 2nd time a different way\n",
    "        except json.JSONDecodeError as jde:\n",
    "            failed_files.append([json_file, jde])\n",
    "            continue\n",
    "\n",
    "\n",
    "for f in failed_files: \n",
    "    with open(f[0]) as j:\n",
    "        content = j.read().strip()\n",
    "        p = jr.repair_json(content) # use the json_repair module to auto fix the json file\n",
    "        d = json.loads(p)\n",
    "        count_correctly_loaded += put_to_df(d, all_data)\n",
    "        # we can write the corrected json out to file too, if we wanted to fix source\n",
    "\n",
    "\n",
    "# effectively continue debugging potential file errors until an acceptable amount of data is inside the dataframe. display for a human below.\n",
    "\n",
    "print(f'There are {count_total_files}, of which {count_json_files} are json. {count_correctly_loaded} ingested to dataframe succesfully, {count_cannot_load} could not load.')\n",
    "print(f'The column shape of the json files: {file_shape}')\n",
    "\n",
    "\n",
    "# convert all the json dict objects to a pandas dataframe for easier processing.\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "print(f'Double check DF length: {len(df)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 'd088da63-558d-4008-b378-fde896abafd8', 'name': '???', 'email': 'invalid-email', 'instagram_handle': None, 'tiktok_handle': '#error_handle', 'joined_at': 'not-a-date', 'advocacy_programs': [{'program_id': '', 'brand': 12345, 'tasks_completed': [{'task_id': None, 'platform': 123, 'post_url': 'https://fruitful-mathematics.info/', 'likes': 125, 'comments': None, 'shares': 52, 'reach': 9055}], 'total_sales_attributed': 142.46206171850474}]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>instagram_handle</th>\n",
       "      <th>tiktok_handle</th>\n",
       "      <th>joined_at</th>\n",
       "      <th>advocacy_programs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9778</th>\n",
       "      <td>d088da63-558d-4008-b378-fde896abafd8</td>\n",
       "      <td>???</td>\n",
       "      <td>invalid-email</td>\n",
       "      <td>None</td>\n",
       "      <td>#error_handle</td>\n",
       "      <td>not-a-date</td>\n",
       "      <td>[{'program_id': '', 'brand': 12345, 'tasks_com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   user_id name          email  \\\n",
       "9778  d088da63-558d-4008-b378-fde896abafd8  ???  invalid-email   \n",
       "\n",
       "     instagram_handle  tiktok_handle   joined_at  \\\n",
       "9778             None  #error_handle  not-a-date   \n",
       "\n",
       "                                      advocacy_programs  \n",
       "9778  [{'program_id': '', 'brand': 12345, 'tasks_com...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proof that a failed json file (user_7100 below) ingested into the data objects correctly.\n",
    "for x in all_data:\n",
    "    if x['user_id'] == 'd088da63-558d-4008-b378-fde896abafd8':\n",
    "        print(x)\n",
    "\n",
    "df.query('user_id==\"d088da63-558d-4008-b378-fde896abafd8\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>instagram_handle</th>\n",
       "      <th>tiktok_handle</th>\n",
       "      <th>joined_at</th>\n",
       "      <th>advocacy_programs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157ad894-ea7c-4201-b3a3-04cced2b55f3</td>\n",
       "      <td>Mona Grimes</td>\n",
       "      <td>Ethelyn.Kertzmann@yahoo.com</td>\n",
       "      <td>@Amira.Russel71</td>\n",
       "      <td>@Novella87</td>\n",
       "      <td>2025-03-04T01:42:42.916Z</td>\n",
       "      <td>[{'program_id': 'da0afce8-2b3b-4c36-af7c-4df3a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2bcf3b05-8362-4eb2-a04d-254ff9442365</td>\n",
       "      <td>Mary Kiehn</td>\n",
       "      <td>Rolando48@gmail.com</td>\n",
       "      <td>@Earnestine98</td>\n",
       "      <td>@Darrion40</td>\n",
       "      <td>2024-03-28T08:19:52.264Z</td>\n",
       "      <td>[{'program_id': '0a548308-13e9-45a0-9335-2a025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>???</td>\n",
       "      <td>invalid-email</td>\n",
       "      <td>None</td>\n",
       "      <td>#error_handle</td>\n",
       "      <td>2025-01-21T00:11:39.358Z</td>\n",
       "      <td>[{'program_id': '', 'brand': 12345, 'tasks_com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>???</td>\n",
       "      <td>invalid-email</td>\n",
       "      <td>None</td>\n",
       "      <td>#error_handle</td>\n",
       "      <td>not-a-date</td>\n",
       "      <td>[{'program_id': '', 'brand': 12345, 'tasks_com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>???</td>\n",
       "      <td>invalid-email</td>\n",
       "      <td>None</td>\n",
       "      <td>#error_handle</td>\n",
       "      <td>not-a-date</td>\n",
       "      <td>[{'program_id': '', 'brand': 12345, 'tasks_com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id         name  \\\n",
       "0  157ad894-ea7c-4201-b3a3-04cced2b55f3  Mona Grimes   \n",
       "1  2bcf3b05-8362-4eb2-a04d-254ff9442365   Mary Kiehn   \n",
       "2                                  None          ???   \n",
       "3                                  None          ???   \n",
       "4                                  None          ???   \n",
       "\n",
       "                         email instagram_handle  tiktok_handle  \\\n",
       "0  Ethelyn.Kertzmann@yahoo.com  @Amira.Russel71     @Novella87   \n",
       "1          Rolando48@gmail.com    @Earnestine98     @Darrion40   \n",
       "2                invalid-email             None  #error_handle   \n",
       "3                invalid-email             None  #error_handle   \n",
       "4                invalid-email             None  #error_handle   \n",
       "\n",
       "                  joined_at                                  advocacy_programs  \n",
       "0  2025-03-04T01:42:42.916Z  [{'program_id': 'da0afce8-2b3b-4c36-af7c-4df3a...  \n",
       "1  2024-03-28T08:19:52.264Z  [{'program_id': '0a548308-13e9-45a0-9335-2a025...  \n",
       "2  2025-01-21T00:11:39.358Z  [{'program_id': '', 'brand': 12345, 'tasks_com...  \n",
       "3                not-a-date  [{'program_id': '', 'brand': 12345, 'tasks_com...  \n",
       "4                not-a-date  [{'program_id': '', 'brand': 12345, 'tasks_com...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the data set look like?\n",
    "df.head()\n",
    "\n",
    "# inspect and debug with more clarity with Datawrangler extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the dataset look like?\n",
    "- We have reasonably well structured data, although many fields have poor health that will have to be handled later.\n",
    "- We have 6 consistent fields, in user_id, name, email, instagram_handle, tiktok_handle.\n",
    "  - Later in dbt, Lets check for rows with nonsense data, e.g. no PII, no useful keys, and should be purged. \n",
    "  - We have a nested data structure in advocacy_programs, lets unpack at least that into the main dataframe...\n",
    "- Normalise to per platform for easier mapping and nicer structure?\n",
    "  - At this point, migrate it out of pandas.\n",
    "- Explore duplicated user ID's - get most recent?\n",
    "  - Implement dbt snapshot sorted by the date stamp?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets unpack advocacy program into the parent.\n",
    "unpacked = pd.json_normalize(df['advocacy_programs'].explode('advocacy_programs'))\n",
    "newdf = pd.concat([df, unpacked], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tasks = pd.json_normalize(newdf['tasks_completed'].explode('tasks_completed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a docker db for uploading to : postgres?\n",
    "- unpack tasks completed into a seperate table with userid\n",
    "- clean the data up\n",
    "- implement the dbt process - connect to postgres\n",
    "- sort the dockerisation process out for easy reproduction\n",
    "- Airflow task\n",
    "    - Read the files\n",
    "    - tidy them\n",
    "    - upload to postgres\n",
    "    - run any dbt packages\n",
    "    - create a simple export file from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the data somewhere so we can use it in dbt-core: \n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# connect and use the local postgres instance we have running in docker.\n",
    "engine = create_engine('postgresql://postgres:pass@127.0.0.1:5432/duel')\n",
    "\n",
    "# this is safe to do in this instance, because advocacy_programs always has only 1 item in it.\n",
    "newdf[['user_id', 'name', 'email', 'instagram_handle', 'tiktok_handle', 'joined_at']].astype(str).to_sql('source', engine, if_exists='replace', index=True, method='multi', schema='raw')\n",
    "newdf[['program_id', 'brand', 'total_sales_attributed']].astype(str).to_sql('source__advocacy_programs', engine, if_exists='replace', index=True, method='multi', schema='raw')\n",
    "tasks.astype(str).to_sql('source__tasks_completed', engine, if_exists='replace', index=True, method='multi', schema='raw')\n",
    "\n",
    "# df.astype(str).to_sql('source', engine, if_exists='replace', index=True, method='multi', schema='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider this stage done. data has been ingested and viewed at a basic level. \n",
    "\n",
    "The data has been expanded first in python for faster and easier json error handling and unpacking. If these json lists were of length > 1, we could handle that by duplicating the index id and having a one to many relationship. However, all lists are length 1 in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets capture some rows that probably should be excluded.\n",
    "#  e.g. if there's no handle information, it might not be the most useful data - but we can still keep it incase there are some insights inside.\n",
    "\n",
    "\n",
    "# missing_handles = newdf.query('user_id != user_id and email == \"invalid-email\" and instagram_handle != instagram_handle and tiktok_handle == \"#error_handle\"')\n",
    "\n",
    "# cleaned = newdf.drop(missing_handles.index)\n",
    "# cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
